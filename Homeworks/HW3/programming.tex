\newpage

\section*{Part C: Implementing SVM Variation [20 points] (Yichong and Prakhar) }
\textbf{Please attach your code as appendix to this problem.}\\
In the last homework, we derived a variant of SVM that explicitly maximizes the margin.
You can use any library on quadratic optimization (e.g., CVXOPT for python or \textsf{quadprog} for MATLAB) for this problem. Here is an instruction on CVXOPT.

Installation instructions of CVXOPT can be found \href{http://cvxopt.org/install/index.html}{here}. CVXOPT provides an easy interface for quadratic programming: The function \textsf{qp(P, q[, G, h[, A, b]])} solves the optimization problem
\begin{align*}
\text{minimize}_{x \in \mathbb{R}^n}\;  & (1/2) x^TPx+q^Tx \\
\text{subject to } & Gx \preceq h \\
& Ax=b
\end{align*}
for $P\in \mathbb{R}^{n\times n}, q\in \mathbb{R}^n, G\in \mathbb{R}^{m_1\times n}, h \in \mathbb{R}^{m_1}, A \in \mathbb{R}^{m_2\times n}, b \in \mathbb{R}^{m_2}$.
Here $\preceq$ means pointwise less than or equal; i.e., if $a \preceq b $ for $a,b \in \mathbb{R}^n$, then $a_i\leq b_i \forall i=1,2,...,n$, where the subscript indicates coordinates. Look into \href{http://cvxopt.org/examples/tutorial/qp.html}{here} for a concrete example including all details.

In last homework we want to solve the following primal problem:
\begin{align}
\min_{\boldsymbol{w}, \boldsymbol{\xi}, \rho}&\quad \frac{1}{2} {\boldsymbol{||w||}}_2^2 + \frac{1}{2}b^2- \rho + \frac{\lambda}{2} \sum_{i=1}^n \xi_i^2\label{eqn:primal}\\
\text{subject to} &\quad y_i(w^T \boldsymbol{x_i} + b) \ge \rho - \xi_i,\quad i = 1,\dotsc,n 	\nonumber
\end{align}


We derived a dual program of (\ref{eqn:primal}) in homework 2. (Have a look at the solutions if you did not solve it - we will release it soon.) Implement the dual program using CVXOPT. Compute results for both $\lambda=1$ and $\lambda=10$. The data is a toy dataset *train\_data.csv* in {\color{red}csv} format of size $\mathbb{R}^{100\times 2}$, and the training label is contained in the file *train\_label.csv* of size $\mathbb{R}^{100\times 1}$. Be careful that CVXOPT uses a slightly different matrix format than numpy; so either create your matrix in CVXOPT format, or use a numpy array and convert it into CVXOPT format using \textsf{A = cvxopt.matrix(A)}. \\
(a) \textbf{[4 points]} Suppose you use $\boldsymbol{\alpha}$ as the Lagrange multipliers in dual program. Given the dual solution $\boldsymbol{\alpha}$, compute the primal solution $\boldsymbol{w}, \boldsymbol{\xi}, \rho$ in terms of training data, $\lambda$ and $\boldsymbol{\alpha}$. (Hint: This has been computed in last homework, and you do not need to redo them.) \\
(b) \textbf{[8 points]} For each value of $\lambda$, draw a scatter plot of the data and plot the decision border  (where
the predicted class label changes) as well as the boundaries of the margin (the area in which
there is a nonzero penalty for predicting any label). Use different colors for margins and border. Also use different colors for positive ($y=1$) and negative ($y=-1$) samples.\\
(c) \textbf{[4 points]} Report the test error on the test set *test\_data.csv* and *test\_label.csv* for each value of $\lambda$.\\
(d) \textbf{[4 points]} What is the difference between the result obtained in $\lambda=1$ and $\lambda=10$? Why is that?

\newpage
