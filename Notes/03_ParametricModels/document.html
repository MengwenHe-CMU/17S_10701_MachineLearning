<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Praddep Ravikumar (Instructor), HMW-Alexander (Noter)">
  <title>Parametric Models: Prior Information, From Models to Answers</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/x-mathjax-config">
  	MathJax.Hub.Config({
  		TeX: {
  			equationNumbers: {autoNumber: "all"}
  		}
  	});
  </script>
</head>
<body>
<header>
<h1 class="title"><strong>Parametric Models: Prior Information, From Models to Answers</strong></h1>
<p class="author">Praddep Ravikumar (Instructor), HMW-Alexander (Noter)</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#resources">Resources</a></li>
<li><a href="#bayesian-learning"><span class="toc-section-number">1</span> Bayesian Learning</a><ul>
<li><a href="#prior-distribution"><span class="toc-section-number">1.1</span> Prior Distribution</a><ul>
<li><a href="#where-to-get"><span class="toc-section-number">1.1.1</span> Where to get</a></li>
<li><a href="#uniformative-priors"><span class="toc-section-number">1.1.2</span> Uniformative priors</a></li>
<li><a href="#conjugate-priors"><span class="toc-section-number">1.1.3</span> Conjugate Priors</a></li>
</ul></li>
</ul></li>
<li><a href="#maximum-a-posteriori-estimation"><span class="toc-section-number">2</span> Maximum A Posteriori Estimation</a><ul>
<li><a href="#mle-vs.-map"><span class="toc-section-number">2.1</span> MLE vs. MAP</a></li>
<li><a href="#map-for-gaussian-mean-and-variance"><span class="toc-section-number">2.2</span> MAP for Gaussian mean and variance</a></li>
</ul></li>
<li><a href="#non-bayesian-prior-information-via-constraints"><span class="toc-section-number">3</span> Non-Bayesian Prior Information via Constraints</a></li>
</ul>
</nav>
<hr />
<p><a href="../index.html">Back to Index</a></p>
<hr />
<h1 id="resources" class="unnumbered">Resources</h1>
<ul>
<li><p><a href="../../Lectures/03_ParametricModels.pdf">Lecture</a></p></li>
</ul>
<hr />
<h1 id="bayesian-learning"><span class="header-section-number">1</span> Bayesian Learning</h1>
<p>Given a prior knowledge to estimate the model.</p>
<p>Bayesian Learning: <span class="math display">\[P(\theta|\mathcal{D}) = \frac{P(\mathcal{D}|\theta)P(\theta)}{P(\mathcal{D})}\]</span> or equivalently <span class="math display">\[P(\theta|\mathcal{D}) \propto P(\mathcal{D}|\theta)P(\theta)\]</span> Likelihood measures the fitness between data and parameters, Prior is the knowledge how possible the parameters to be.</p>
<ul>
<li><p>Prior information encoded as a distribution over possible values of parameter.</p></li>
<li><p>Using the Bayes rule to get an updated posterior distribution over parameters.</p></li>
</ul>
<h2 id="prior-distribution"><span class="header-section-number">1.1</span> Prior Distribution</h2>
<h3 id="where-to-get"><span class="header-section-number">1.1.1</span> Where to get</h3>
<ul>
<li><p>Represents expert knowledge (philosophical approach)</p></li>
<li><p>Simple posterior form (engineer’s approach)</p></li>
</ul>
<h3 id="uniformative-priors"><span class="header-section-number">1.1.2</span> Uniformative priors</h3>
<p>Simple distribution.</p>
<p><img src="./img/uniform.png" alt="image" width="151" /></p>
<h3 id="conjugate-priors"><span class="header-section-number">1.1.3</span> Conjugate Priors</h3>
<ul>
<li><p>Closed-form representation of posterior</p></li>
<li><p>prior and posterior have the same algebraic form as a function of parameters</p></li>
</ul>
<p>Bernoulli Example: (Binomial’s conjugate prior is Beta distribution)</p>
<ul>
<li><p>Likelihood in Bernoulli model: <span class="math inline">\(P(D|\theta)=\theta^{\alpha_1}(1-\theta)^{\alpha_2}\)</span></p></li>
<li><p>Prior is Beta distribution: <span class="math inline">\(P(\theta)=\frac{\theta^{\beta_1-1}(1-\theta)^{\beta_2-1}}{B(\beta_1,\beta_2)} \sim Beta(\beta_1,\beta_2)\)</span></p></li>
<li><p>Posterior is also Beta distribution: <span class="math inline">\(P(\theta|D)\sim Beta(\beta_1+\alpha_1,\beta_2+\alpha_2)\)</span></p></li>
</ul>
<p>Multinomial example: (Multinomial’s conjugate prior is Dirichelet distribution)</p>
<ul>
<li><p>Likelihood is Multinomial(<span class="math inline">\(\theta=\{\theta_1,\dots,\theta_k\}\)</span>), <span class="math inline">\(P(D|\theta)=\prod_{i=1}^{k}\theta_i^{\alpha_i}\)</span>, <span class="math inline">\(\alpha_i\in\{0,1\}\)</span> is the data <span class="math inline">\(D\)</span>, <span class="math inline">\(\sum_{i=1}^{k}\theta_i =1\)</span>.</p></li>
<li><p>Prior is Dirichlet distribution: <span class="math inline">\(P(\theta)=\frac{\prod_{i=1}^{k}\theta_i^{\beta_i-1}}{B(\beta_1,\dots,\beta_k)} \sim Dirichlet(\beta_1,\dots,\beta_k)\)</span></p></li>
<li><p>Posterior is also dirichlet distribution: <span class="math inline">\(P(\theta|D) \sim Dirichlet(\beta_1+\alpha_1,\dots,\beta_k+\alpha_k)\)</span></p></li>
</ul>
<p>As we get more samples, effect of prior is “washed out”</p>
<h1 id="maximum-a-posteriori-estimation"><span class="header-section-number">2</span> Maximum A Posteriori Estimation</h1>
<p>Choose <span class="math inline">\(\theta\)</span> that maximizes a posterior probability: <span class="math inline">\(\hat{\theta}_{MAP}=\arg\max_\theta{P(\theta|D)}\)</span></p>
<p><span class="math display">\[\begin{array}{rcl}
\hat{\theta}_{MAP} &amp; = &amp; \arg\max_\theta{P(\theta|D)} \\
                   &amp; = &amp; \arg\max_\theta{P(D|\theta)P(\theta)}
\end{array}\]</span></p>
<p>Bernoulli example:</p>
<p><span class="math display">\[\begin{array}{rcl}
P(\theta|D) &amp; \sim &amp; Beta(\beta_1+\alpha_1,\beta_2+\beta_2) \\
\hat{\theta}_{MAP} &amp; = &amp; \frac{\alpha_1+\beta_1-1}{\alpha_1+\beta_1+\alpha_2+\beta_2-2}
\end{array}\]</span></p>
<h2 id="mle-vs.-map"><span class="header-section-number">2.1</span> MLE vs. MAP</h2>
<ul>
<li><p>MLE: Choose value that maximizes the probability of observed data</p></li>
<li><p>MAP: Choose value that is mot probable given observed data and prior belief</p></li>
<li><p>When prior is a uniform distribution, MLE=MAP.</p></li>
</ul>
<h2 id="map-for-gaussian-mean-and-variance"><span class="header-section-number">2.2</span> MAP for Gaussian mean and variance</h2>
<p>Conjugate priors</p>
<ul>
<li><p>Gaussian prior: <span class="math display">\[P(\mu|\eta,\lambda)=\frac{1}{\lambda\sqrt{2\pi}}\exp(-\frac{(\mu-\eta)^2}{2\lambda^2})=\mathcal{N}(\eta,\lambda)\]</span></p></li>
<li><p>Variance: Wishart Distribution<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p></li>
</ul>
<p>MAP for Gasussian Mean:</p>
<ul>
<li><p><span class="math inline">\(\hat{\mu}_{MLE}=\frac{1}{n}\sum_{i=1}^{n}x_i\)</span></p></li>
<li><p><span class="math inline">\(\hat{\mu}_{MAP}=\frac{\frac{1}{\sigma^2}\sum_{i=1}^{n}x_i+\frac{\eta}{\lambda^2}}{\frac{n}{\sigma^2}+\frac{1}{\lambda^2}}\)</span></p></li>
</ul>
<h1 id="non-bayesian-prior-information-via-constraints"><span class="header-section-number">3</span> Non-Bayesian Prior Information via Constraints</h1>
<ul>
<li><p>MLE: <span class="math display">\[\max_\theta\log P(D|\theta)\]</span></p></li>
<li><p>Constrained MLE: <span class="math display">\[\max _\theta\log P(D|\theta)~~s.t.~\mathcal{R}(\theta)\leq C\]</span></p></li>
<li><p>When <span class="math inline">\(\mathcal{R}\)</span> is convex, constrained MLE is equivalent to regularized MLE (lagrange multiplier<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>): <span class="math display">\[\max_\theta\{\log P(D|\theta)+\lambda\mathcal{R}(\theta)\}\]</span></p></li>
<li><p>The MAP estimator can be seen to be a special case by simply setting: <span class="math display">\[\lambda\mathcal{R}(\theta)=\log P(\theta)\]</span></p></li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Wishart_distribution" class="uri">https://en.wikipedia.org/wiki/Wishart_distribution</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://www1.maths.leeds.ac.uk/~cajones/math2640/notes4.pdf" class="uri">http://www1.maths.leeds.ac.uk/~cajones/math2640/notes4.pdf</a><a href="#fnref2">↩</a></p></li>
</ol>
</section>
</body>
</html>
