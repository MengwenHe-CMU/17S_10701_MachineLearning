\documentclass[letterpaper,10pt]{article}
\usepackage[margin=2cm]{geometry}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[colorlinks]{hyperref}

\newcommand{\panhline}{\begin{center}\rule{\textwidth}{1pt}\end{center}}

\title{\textbf{Linear Algebra}}
\author{Danish (Instructor), HMW-Alexander (Noter)}

\begin{document}

\maketitle

\panhline
\href{../index.html}{Back to Index}

\panhline
\tableofcontents

\section*{Resources}

\begin{itemize}
	\item \href{../../Lectures/R2_LinearAlgebra.pdf}{Lecture}
\end{itemize}

\panhline

\section{Vector Spaces}

A vector space ($V$) is a collection of objects called vectors, which may be added together and multiplied by numbers, called scalars in this context.

\section{Trace}

$$tr: \mathbb{R}^{n\times n} \rightarrow \mathbb{R}$$
$$tr(A)=\sum_{i=1}^{n}A_{ii}$$

\begin{itemize}
	\item $tr(A)=tr(A^T)$
	\item $tr(A+B)=tr(A)+tr(B)$
	\item $tr(AB)=tr(BA)$
\end{itemize}

\section{Norms}

A vector norm is any function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ with:
\begin{itemize}
	\item $f(x)\geq 0$ and $f(x)=0 \Leftrightarrow x=0$
	\item $f(ax)=|a|f(x)$ for $a\in \mathbb{R}$
	\item $f(x+y)\leq f(x)+f(y)$
\end{itemize}

Norms of vectors:
\begin{itemize}
	\item $l_2$: $$||x||_2=\sqrt{x^Tx}=\sqrt{\sum{x_i^2}}$$
	\item $l_1$: $$||x||_1=\sum|x_i|$$
	\item $l_\infty$: $$||x||_\infty=\max(|x_i|)$$
\end{itemize}

Geometric interpretation:
\begin{figure}[!h]
	\centering
	\includegraphics[width=2cm]{./img/norms.png}
\end{figure}

Norms of Matrix:

$$||A||_p=\sup_{x\neq0}\frac{||A\vec{x}||_p}{||x||_p}$$
\begin{itemize}
	\item $||A_{m\times n}||_1=\max_{1\leq j \leq n}\sum_{i=1}^{m}|a_{ij}|$, which is simply the maximum absolute column sum of the matrix.
	\item $||A_{m\times n}||_\infty=\max_{1\leq i \leq m}\sum_{j=1}^{n}|a_{ij}|$, which is simply the maximum absolute row sum of the matrix.
	\item $||A_{m\times n}||_2 = \sqrt{\lambda_{max}(A^*A)}=\sigma_{max}(A)$ (spectral norm)
	\item $||A_{m\times n}||_2 \leq (\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|^2)^{1/2}=||A||_F$ (Frobenious norm, $l_{2,2}$)
\end{itemize}
Generally, the subordinate matrix norm on $K_{m\times n}$ induced by $||\cdot||_\alpha$ on $K_n$, and $||\cdot||_\beta$ on $K_m$ as: $$||A||_{\alpha,\beta}=\max_{x\neq 0}\frac{||A\vec{x}||_\beta}{||\vec{x}||_\alpha}$$


\section{The Matrix Inverse}

$$AA^{-1}=I$$

$$A^{-1} \text{ exists } \Leftrightarrow Ax\neq 0 \text{ for all } x\neq 0$$

Properties:
\begin{itemize}
	\item $(A^{-1})^{-1}=A$
	\item $(AB)^{-1}=B^{-1}A^{-1}$
	\item $(A^T)^{-1}=(A^{-1})^T$
\end{itemize}

\section{Linear Independence and Rank}

$$\text{iff }\alpha_i=0, \sum_{i} \alpha_i \vec{x}_i = 0 \Rightarrow \text{ linear independence}$$

RREF (Reduced Row Echlon Form) to get rank of a matrix.

column rank = row rank

\section{Orthogonality}

$$\vec{x}^T\vec{y}=0$$

Orthonormal:$||\vec{x}||_2=1$

A matrix is orthogonal if all it's columns are orthonormal: $U^TU=I$, and its column vectors are linearly independent.

\section{Eigenvalues and Eigenvectors}

$$A\vec{x}=\lambda\vec{x}$$
$$det(\lambda I - A)=0$$

\section{Diagonalization}

For all eigenvectors and eigenvalues, construct:
$$AX=X\Lambda \Rightarrow A=X\lambda X^{-1}$$
If $X$ is invertible, then A is diagonalizable.

Properties of eigenvectors and eigenvalues:
\begin{itemize}
	\item $tr(A)=\sum_i{\lambda_i}$
	\item $det(A)=\prod_i{\lambda_i}$
	\item $rank(A)=$ number of non-zero eigenvalues.
	\item Eigenvalues of $A^{-1}$ are $1/\lambda_i$, and the eigenvectors keep same.
\end{itemize}

\end{document}



