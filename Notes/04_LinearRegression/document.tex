\documentclass[letterpaper,10pt]{article}
\usepackage[margin=2cm]{geometry}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[colorlinks]{hyperref}

\newcommand{\panhline}{\begin{center}\rule{\textwidth}{1pt}\end{center}}

\title{\textbf{Linear Regression}}
\author{Aarti Singh (Instructor), HMW-Alexander (Noter)}

\begin{document}

\maketitle

\panhline
\href{../index.html}{Back to Index}

\panhline
\tableofcontents

\section*{Resources}

\begin{itemize}
	\item \href{../../Lectures/04_LinearRegression.pdf}{Lecture}
\end{itemize}

\panhline

\section{Discrete to Continuous Labels}
	
From classification to regression

\subsection{Task}

Given $X\in \mathcal{X}$, predict $Y \in \mathcal{Y}$, Construct prediction rule $f:\mathcal{X} \rightarrow \mathcal{Y}$

\subsection{Performance Measure}

\begin{itemize}
	\item Quantifies knowledge gained.
	\item Measure of closeness between true label Y and prediction f(X)
	\begin{itemize}
		\item 0/1 lose:$loss(Y,f(X))=1_{f(X)\neq Y}$. Risk: probability of error 
		\item square loss: $loss(Y,f(X))=(f(X)-Y)^2$. Risk: mean square error
	\end{itemize}
	\item How well does the predictor perform on average?
	$$Risk~R(f)=\mathbb{E}[loss(Y,f(X))],~(X,Y)\sim P_{XY}$$
\end{itemize}

\subsection{Bayes Optimal Rule}

\begin{itemize}
	\item ideal goal: Construct prediction rule $f^*:\mathcal{X}\rightarrow\mathcal{Y}$
	$$f^*=\arg\min_f{E_{XY}[loss(Y,f(X))]}$$ (Bayes optimal rule)
	\item Best possible performance:
	$$\forall f,~R(f^*) \leq R(f)$$ (Bayes Risk)
\end{itemize}

Problem: $P_{XY}$ is unknown.

Solution: Training data provides a glimpse of $P_{XY}$
$$\text{(observed)~}\{(X_i,Y_i)\} \sim_{i.i.d} P_{XY}\text{~unknown}$$

\section{Macine Learning Algortihm}

\begin{itemize}
	\item Model based approach: use data to learn a model for $P_{XY}$
	\item Model-free approach: use data to learn mapping directly
\end{itemize}

\subsection{Empirical Risk Minimization (model-free)}

\begin{itemize}
	\item Optimal predictor: $$f^*=\arg\min_f{\mathbb{E}[(f(X)-Y)^2]}$$
	\item Empirical Minimizer: $$\hat{f}_n=\arg\min_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}(f(X)-Y)^2$$
\end{itemize}

$\mathcal{F}$ is the class of predictors:
\begin{itemize}
	\item Linear
	\item Polynomial
	\item Nonlinear
\end{itemize}

\section{Linear Regression}

$$f(\vec{X})=\sum_{i=0}^{p}{\beta_0X^{i}}=\vec{X}^T\vec{\beta},~where~X^0=1,~\vec{\beta}=[\beta_0,\dots,\beta_p]^T$$

$$\hat{\vec{\beta}}=\arg\min_{\vec{\beta}}(A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y}),~where~A=[\vec{X_1},\dots,\vec{X_n}]$$

$$J(\beta)=(A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y})$$

\begin{equation*}
\begin{array}{rcl}
\frac{\partial J(\vec{\beta})}{\partial \vec{\beta}} & = & \frac{\partial (A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y})}{\partial \vec{\beta}} \\
& = & \frac{\partial (\vec{\beta}^TAA^T\vec{\beta}-\vec{\beta}^TA\vec{Y}-\vec{Y}^TA^T\vec{\beta}+\vec{Y}^T\vec{Y})}{\vec{\beta}} \\
& = & (AA^T+(AA^T)^T)\vec{\beta}-A\vec{Y}-A\vec{Y} \\
& = & 2AA^T\vec{\beta}-2A\vec{Y} = 0 \\
& \Rightarrow & AA^T\vec{\beta}=A\vec{Y} \\
& \Rightarrow & \hat{\vec{\beta}}=(AA^T)^{-1}A\vec{Y},~\text{if $AA^T$ is invertible}
\end{array}
\end{equation*}

\subsection{Gradient Descent}

Even when $AA^T$ is invertible, might be computationally expensive if $A$ is huge; however, $J(\vec{\beta})$ is convex\footnote{A function is called convex if the line joining any two points on the function does not go below the function on the interval formed by these two points.} in $\beta$.

Minimum of a convex function can be reached by gradient descent algorithm:
\begin{itemize}
	\item Initialize: pick $\vec{w}$ at random
	\item Gradient: $$\nabla_{\vec{w}} l(\vec{w})=[\frac{\partial l(\vec{w})}{\partial w_0},\dots,\frac{\partial l(\vec{w})}{\partial w_d}]^T$$
	\item Update rule: $$\Delta \vec{w}=\eta \nabla_{\vec{w}}l(\vec{w})$$, $$w_i^{t+1} \leftarrow w_i^t - \eta \frac{\partial l(\vec{w})}{\partial w_i}|_t$$
	\item Stop: when some criterion met $\frac{\partial l(\vec{w})}{\partial w_i}|_t < \epsilon$
\end{itemize}



\subsection{If $AA^T$ is not invertible}

$Rank(AA^T)$ = number of non-zero eigenvalues of $AA^T$ = number of non-zero singular values of A $\leq \min(n,p)$ since $A$ is $n\times p$

$$A=U \Sigma V^T \Rightarrow AA^T=U\Sigma^2U^T \Rightarrow AA^T U = U\Sigma^2$$

\subsubsection{Regularized Leasts Squares}

Ridge Regression (l2 penalty)

\begin{equation}
\begin{array}{rcl}
\hat{\vec{\beta}}_{MAP} & = & \arg\min_{\vec{\beta}}(A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y}) +\lambda \vec{\beta}^T\vec{\beta}~~(\lambda \geq 0) \\
& = & (AA^T + \lambda I)^{-1} A\vec{Y}
\end{array}
\end{equation}

$(AA^T + \lambda I)$ is invertible if $\lambda > 0$. Proof:
\begin{itemize}
	\item the symmetric matrix $AA^T$ is positive-semidefinite matrix, because a matrix is positive-semidefinite iff it arises as the Gram matrix of some set of vectors\footnote{In contrast to the positive-definite case, these vectors need not be linearly independent.}.
	\item $\therefore \forall \lambda>0~and~\vec{x}\neq\vec{0}$, 
	$$\vec{x}^T(AA^T)\vec{x} = (A^T\vec{x})^T(A^T\vec{x}) \geq 0$$
	$$\vec{x}^T(AA^T+\lambda I)\vec{x} = \vec{x}^T (AA^T) \vec{x} + \lambda \vec{x}^T\vec{x} >0$$
	\item $\therefore$ $(AA^T+\lambda I)$ is positive definite.
	\item $\therefore$ the eigenvalues of $B=(AA^T+\lambda I)$ are all positive. $$B\vec{v}=\lambda\vec{v} \Rightarrow \vec{v}^T B \vec{v} = \lambda >0$$
	\item $\therefore$ $(AA^T + \lambda I)$ is invertible if $\lambda > 0$
\end{itemize}

\end{document}



