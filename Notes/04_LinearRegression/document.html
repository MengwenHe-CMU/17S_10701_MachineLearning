<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Aarti Singh (Instructor), HMW-Alexander (Noter)">
  <title>Linear Regression</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/x-mathjax-config">
  	MathJax.Hub.Config({
  		TeX: {
  			equationNumbers: {autoNumber: "all"}
  		}
  	});
  </script>
</head>
<body>
<header>
<h1 class="title"><strong>Linear Regression</strong></h1>
<p class="author">Aarti Singh (Instructor), HMW-Alexander (Noter)</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#resources">Resources</a></li>
<li><a href="#discrete-to-continuous-labels"><span class="toc-section-number">1</span> Discrete to Continuous Labels</a><ul>
<li><a href="#task"><span class="toc-section-number">1.1</span> Task</a></li>
<li><a href="#performance-measure"><span class="toc-section-number">1.2</span> Performance Measure</a></li>
<li><a href="#bayes-optimal-rule"><span class="toc-section-number">1.3</span> Bayes Optimal Rule</a></li>
</ul></li>
<li><a href="#macine-learning-algortihm"><span class="toc-section-number">2</span> Macine Learning Algortihm</a><ul>
<li><a href="#empirical-risk-minimization-model-free"><span class="toc-section-number">2.1</span> Empirical Risk Minimization (model-free)</a></li>
</ul></li>
<li><a href="#linear-regression"><span class="toc-section-number">3</span> Linear Regression</a><ul>
<li><a href="#gradient-descent"><span class="toc-section-number">3.1</span> Gradient Descent</a></li>
<li><a href="#if-aat-is-not-invertible"><span class="toc-section-number">3.2</span> If <span class="math inline">\(AA^T\)</span> is not invertible</a><ul>
<li><a href="#regularized-leasts-squares"><span class="toc-section-number">3.2.1</span> Regularized Leasts Squares</a></li>
<li><a href="#understanding-regularized-least-squared"><span class="toc-section-number">3.2.2</span> Understanding Regularized Least Squared</a></li>
</ul></li>
<li><a href="#regularized-least-squares---connection-to-mle-and-map-model-based-approaches"><span class="toc-section-number">3.3</span> Regularized Least Squares - Connection to MLE and MAP (Model-based Approaches)</a><ul>
<li><a href="#least-squares-and-mcle-maximum-conditional-likelihood-estimator"><span class="toc-section-number">3.3.1</span> Least Squares and M(C)LE (Maximum Conditional Likelihood Estimator)</a></li>
<li><a href="#regularized-least-squares-and-mcap-maximum-conditional-a-prior-estimator"><span class="toc-section-number">3.3.2</span> Regularized Least Squares and M(C)AP (Maximum Conditional A Prior Estimator)</a></li>
</ul></li>
</ul></li>
<li><a href="#polynomial-regression"><span class="toc-section-number">4</span> Polynomial Regression</a><ul>
<li><a href="#bias---vairance-tradeoff"><span class="toc-section-number">4.1</span> Bias - Vairance Tradeoff</a></li>
</ul></li>
<li><a href="#regression-with-basis-functions-or-nonlinear-features"><span class="toc-section-number">5</span> Regression with Basis Functions or Nonlinear Features</a></li>
</ul>
</nav>
<hr />
<p><a href="../index.html">Back to Index</a></p>
<hr />
<h1 id="resources" class="unnumbered">Resources</h1>
<ul>
<li><p><a href="../../Lectures/04_LinearRegression.pdf">Lecture</a></p></li>
</ul>
<hr />
<h1 id="discrete-to-continuous-labels"><span class="header-section-number">1</span> Discrete to Continuous Labels</h1>
<p>From classification to regression</p>
<h2 id="task"><span class="header-section-number">1.1</span> Task</h2>
<p>Given <span class="math inline">\(X\in \mathcal{X}\)</span>, predict <span class="math inline">\(Y \in \mathcal{Y}\)</span>, Construct prediction rule <span class="math inline">\(f:\mathcal{X} \rightarrow \mathcal{Y}\)</span></p>
<h2 id="performance-measure"><span class="header-section-number">1.2</span> Performance Measure</h2>
<ul>
<li><p>Quantifies knowledge gained.</p></li>
<li><p>Measure of closeness between true label Y and prediction f(X)</p>
<ul>
<li><p>0/1 lose:<span class="math inline">\(loss(Y,f(X))=1_{f(X)\neq Y}\)</span>. Risk: probability of error</p></li>
<li><p>square loss: <span class="math inline">\(loss(Y,f(X))=(f(X)-Y)^2\)</span>. Risk: mean square error</p></li>
</ul></li>
<li><p>How well does the predictor perform on average? <span class="math display">\[Risk~R(f)=\mathbb{E}[loss(Y,f(X))],~(X,Y)\sim P_{XY}\]</span></p></li>
</ul>
<h2 id="bayes-optimal-rule"><span class="header-section-number">1.3</span> Bayes Optimal Rule</h2>
<ul>
<li><p>ideal goal: Construct prediction rule <span class="math inline">\(f^*:\mathcal{X}\rightarrow\mathcal{Y}\)</span> <span class="math display">\[f^*=\arg\min_f{E_{XY}[loss(Y,f(X))]}\]</span> (Bayes optimal rule)</p></li>
<li><p>Best possible performance: <span class="math display">\[\forall f,~R(f^*) \leq R(f)\]</span> (Bayes Risk)</p></li>
</ul>
<p>Problem: <span class="math inline">\(P_{XY}\)</span> is unknown.</p>
<p>Solution: Training data provides a glimpse of <span class="math inline">\(P_{XY}\)</span> <span class="math display">\[\text{(observed)~}\{(X_i,Y_i)\} \sim_{i.i.d} P_{XY}\text{~unknown}\]</span></p>
<h1 id="macine-learning-algortihm"><span class="header-section-number">2</span> Macine Learning Algortihm</h1>
<ul>
<li><p>Model based approach: use data to learn a model for <span class="math inline">\(P_{XY}\)</span></p></li>
<li><p>Model-free approach: use data to learn mapping directly</p></li>
</ul>
<h2 id="empirical-risk-minimization-model-free"><span class="header-section-number">2.1</span> Empirical Risk Minimization (model-free)</h2>
<ul>
<li><p>Optimal predictor: <span class="math display">\[f^*=\arg\min_f{\mathbb{E}[(f(X)-Y)^2]}\]</span></p></li>
<li><p>Empirical Minimizer: <span class="math display">\[\hat{f}_n=\arg\min_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}(f(X)-Y)^2\]</span></p></li>
</ul>
<p><span class="math inline">\(\mathcal{F}\)</span> is the class of predictors:</p>
<ul>
<li><p>Linear</p></li>
<li><p>Polynomial</p></li>
<li><p>Nonlinear</p></li>
</ul>
<h1 id="linear-regression"><span class="header-section-number">3</span> Linear Regression</h1>
<p><span class="math display">\[f(\vec{X})=\sum_{i=0}^{p}{\beta_0X^{i}}=\vec{X}^T\vec{\beta},~where~X^0=1,~\vec{\beta}=[\beta_0,\dots,\beta_p]^T\]</span></p>
<p><span class="math display">\[\hat{\vec{\beta}}=\arg\min_{\vec{\beta}}(A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y}),~where~A=[\vec{X_1},\dots,\vec{X_n}]\]</span></p>
<p><span class="math display">\[J(\beta)=(A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y})\]</span></p>
<p><span class="math display">\[\begin{array}{rcl}
\frac{\partial J(\vec{\beta})}{\partial \vec{\beta}} &amp; = &amp; \frac{\partial (A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y})}{\partial \vec{\beta}} \\
&amp; = &amp; \frac{\partial (\vec{\beta}^TAA^T\vec{\beta}-\vec{\beta}^TA\vec{Y}-\vec{Y}^TA^T\vec{\beta}+\vec{Y}^T\vec{Y})}{\vec{\beta}} \\
&amp; = &amp; (AA^T+(AA^T)^T)\vec{\beta}-A\vec{Y}-A\vec{Y} \\
&amp; = &amp; 2AA^T\vec{\beta}-2A\vec{Y} = 0 \\
&amp; \Rightarrow &amp; AA^T\vec{\beta}=A\vec{Y} \\
&amp; \Rightarrow &amp; \hat{\vec{\beta}}=(AA^T)^{-1}A\vec{Y},~\text{if $AA^T$ is invertible}
\end{array}\]</span></p>
<h2 id="gradient-descent"><span class="header-section-number">3.1</span> Gradient Descent</h2>
<p>Even when <span class="math inline">\(AA^T\)</span> is invertible, might be computationally expensive if <span class="math inline">\(A\)</span> is huge; however, <span class="math inline">\(J(\vec{\beta})\)</span> is convex<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> in <span class="math inline">\(\beta\)</span>.</p>
<p>Minimum of a convex function can be reached by gradient descent algorithm:</p>
<ul>
<li><p>Initialize: pick <span class="math inline">\(\vec{w}\)</span> at random</p></li>
<li><p>Gradient: <span class="math display">\[\nabla_{\vec{w}} l(\vec{w})=[\frac{\partial l(\vec{w})}{\partial w_0},\dots,\frac{\partial l(\vec{w})}{\partial w_d}]^T\]</span></p></li>
<li><p>Update rule: <span class="math display">\[\Delta \vec{w}=\eta \nabla_{\vec{w}}l(\vec{w})\]</span>, <span class="math display">\[w_i^{t+1} \leftarrow w_i^t - \eta \frac{\partial l(\vec{w})}{\partial w_i}|_t\]</span></p></li>
<li><p>Stop: when some criterion met <span class="math inline">\(\frac{\partial l(\vec{w})}{\partial w_i}|_t &lt; \epsilon\)</span></p></li>
</ul>
<h2 id="if-aat-is-not-invertible"><span class="header-section-number">3.2</span> If <span class="math inline">\(AA^T\)</span> is not invertible</h2>
<p><span class="math inline">\(Rank(AA^T)\)</span> = number of non-zero eigenvalues of <span class="math inline">\(AA^T\)</span> = number of non-zero singular values of A <span class="math inline">\(\leq \min(n,p)\)</span> since <span class="math inline">\(A\)</span> is <span class="math inline">\(n\times p\)</span></p>
<p><span class="math display">\[A=U \Sigma V^T \Rightarrow AA^T=U\Sigma^2U^T \Rightarrow AA^T U = U\Sigma^2\]</span></p>
<h3 id="regularized-leasts-squares"><span class="header-section-number">3.2.1</span> Regularized Leasts Squares</h3>
<p>Ridge Regression (L2 penalty)</p>
<p><span class="math display">\[\begin{array}{rcl}
\hat{\vec{\beta}}_{MAP} &amp; = &amp; \arg\min_{\vec{\beta}}(A^T\vec{\beta}-\vec{Y})^T(A^T\vec{\beta}-\vec{Y}) +\lambda \vec{\beta}^T\vec{\beta}~~(\lambda \geq 0) \\
&amp; = &amp; (AA^T + \lambda I)^{-1} A\vec{Y}
\end{array}\]</span></p>
<p><span class="math inline">\((AA^T + \lambda I)\)</span> is invertible if <span class="math inline">\(\lambda &gt; 0\)</span>. Proof:</p>
<ul>
<li><p>the symmetric matrix <span class="math inline">\(AA^T\)</span> is positive-semidefinite matrix, because a matrix is positive-semidefinite iff it arises as the Gram matrix of some set of vectors<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p></li>
<li><p><span class="math inline">\(\therefore \forall \lambda&gt;0~and~\vec{x}\neq\vec{0}\)</span>, <span class="math display">\[\vec{x}^T(AA^T)\vec{x} = (A^T\vec{x})^T(A^T\vec{x}) \geq 0\]</span> <span class="math display">\[\vec{x}^T(AA^T+\lambda I)\vec{x} = \vec{x}^T (AA^T) \vec{x} + \lambda \vec{x}^T\vec{x} &gt;0\]</span></p></li>
<li><p><span class="math inline">\(\therefore\)</span> <span class="math inline">\((AA^T+\lambda I)\)</span> is positive definite.</p></li>
<li><p><span class="math inline">\(\therefore\)</span> the eigenvalues of <span class="math inline">\(B=(AA^T+\lambda I)\)</span> are all positive. <span class="math display">\[B\vec{v}=\lambda\vec{v} \Rightarrow \vec{v}^T B \vec{v} = \lambda &gt;0\]</span></p></li>
<li><p><span class="math inline">\(\therefore\)</span> <span class="math inline">\((AA^T + \lambda I)\)</span> is invertible if <span class="math inline">\(\lambda &gt; 0\)</span></p></li>
</ul>
<h3 id="understanding-regularized-least-squared"><span class="header-section-number">3.2.2</span> Understanding Regularized Least Squared</h3>
<p>Why we need constraints: r equations, p unknowns - underdetermined system of linear equations.</p>
<p><span class="math display">\[\min_{\vec{\beta}} J(\beta)+\lambda pen(\vec{\lambda})\]</span></p>
<ul>
<li><p>Ridge Regression: <span class="math inline">\(pen(\beta)=||\beta||_2^2\)</span></p></li>
<li><p>Lasso Regression: <span class="math inline">\(pen(\beta)=||\beta||_1\)</span>. No closed form solution, but can optimize using sub-gradient descent.</p></li>
<li><p><span class="math inline">\(pen(\beta)=||\beta||_0=\sum 1_{\beta_i \neq 0}\)</span></p></li>
</ul>
<figure>
<img src="./img/ridgeregression.png" alt="For Lasso regression, results are in sparse solution - vector with more zero coordinates. Good for high-dimenstional problems - don’t have to store all coordinates, interpretable solution! " width="377" /><figcaption>For Lasso regression, results are in sparse solution - vector with more zero coordinates. Good for high-dimenstional problems - don’t have to store all coordinates, interpretable solution! </figcaption>
</figure>
<p>Matlab code:</p>
<div class="sourceCode" language="Matlab"><pre class="sourceCode matlab"><code class="sourceCode matlab">[B,FitInfo] = lasso(X,Y,Name,Value)</code></pre></div>
<ul>
<li><p>X: Numeric matrix with n rows and p columns. Each row represents one observation, and each column represents one predictor (variable).</p></li>
<li><p>Y: Numeric vector of length n, where n is the number of rows of X. Y(i) is the response to row i of X.</p></li>
<li><p>’Alpha’: Scalar value from 0 to 1 (excluding 0) representing the weight of lasso (L1) versus ridge (L2) optimization. Alpha = 1 represents lasso regression, Alpha close to 0 approaches ridge regression, and other values represent elastic net optimization. See Definitions. Default: 1</p></li>
</ul>
<h2 id="regularized-least-squares---connection-to-mle-and-map-model-based-approaches"><span class="header-section-number">3.3</span> Regularized Least Squares - Connection to MLE and MAP (Model-based Approaches)</h2>
<h3 id="least-squares-and-mcle-maximum-conditional-likelihood-estimator"><span class="header-section-number">3.3.1</span> Least Squares and M(C)LE (Maximum Conditional Likelihood Estimator)</h3>
<p><span class="math display">\[Y=f^*(X)+\epsilon=X\beta^*+\epsilon\]</span> <span class="math display">\[\epsilon \sim \mathcal{N}(0,\sigma^2I)~~Y\sim\mathcal{N}(X\beta^*,\sigma^2I)\]</span> <span class="math display">\[\hat{\beta}_{MLE} = \arg\max_\beta (\log p(\{Y_i\}|\beta,\sigma^2,\{X_i\}))=\arg\min_{\beta}\sum_i(X_i\beta-Y_i)^2\]</span></p>
<ul>
<li><p>Model parameters: <span class="math inline">\(\beta,\sigma^2\)</span></p></li>
<li><p>Conditional log likelihood: <span class="math inline">\(\log p(\{Y_i\}|\beta,\sigma^2,\{X_i\})\)</span></p></li>
</ul>
<p>Least Square Estimator is same as Maximum Conditional Likelihood Estimator under a Gaussian model.</p>
<h3 id="regularized-least-squares-and-mcap-maximum-conditional-a-prior-estimator"><span class="header-section-number">3.3.2</span> Regularized Least Squares and M(C)AP (Maximum Conditional A Prior Estimator)</h3>
<p>If <span class="math inline">\(AA^T\)</span> is not invertible.</p>
<p><span class="math display">\[Y=f^*(X)+\epsilon=X\beta^*+\epsilon\]</span> <span class="math display">\[\epsilon \sim \mathcal{N}(0,\sigma^2I)~~Y\sim\mathcal{N}(X\beta^*,\sigma^2I)\]</span> (1) Gaussian prior: <span class="math display">\[\beta \sim \mathcal{N}(0,\tau^2 I)~~p(\beta) \propto \exp(-\beta^T\beta/2\tau^2)\]</span> <span class="math display">\[\hat{\beta}_{MAP} = \arg\max_\beta \log p(\{Y_i\}|\beta,\sigma^2,\{X_i\}) +\log p(\beta)=\arg\min_{\beta}\sum_i(X_i\beta-Y_i)^2+\lambda(\sigma^2,\tau^2)||\beta||_2^2\]</span> (2) Laplace prior: <span class="math display">\[\beta \sim Laplace(0,t)~~p(\beta_i) \propto \exp(-|\beta_i|/t)\]</span> <span class="math display">\[\hat{\beta}_{MAP} = \arg\max_\beta \log p(\{Y_i\}|\beta,\sigma^2,\{X_i\}) +\log p(\beta)=\arg\min_{\beta}\sum_i(X_i\beta-Y_i)^2+\lambda(\sigma^2,\tau^2)||\beta||_1\]</span></p>
<ul>
<li><p>Model parameters: <span class="math inline">\(\beta,\sigma^2\)</span></p></li>
<li><p>Conditional log likelihood: <span class="math inline">\(\log p(\{Y_i\}|\beta,\sigma^2,\{X_i\})\)</span></p></li>
<li><p>Log prior: <span class="math inline">\(\log p(\beta)\)</span></p></li>
</ul>
<h1 id="polynomial-regression"><span class="header-section-number">4</span> Polynomial Regression</h1>
<ul>
<li><p>Univariate: <span class="math inline">\(f(X)=\sum{\beta_iX^i}=[1, X, X^2, \dots, X^m]^T\beta\)</span> <span class="math display">\[\hat{\beta}=(AA^T)^{-1}AY~or~(AA^T+\lambda I)^{-1}AY\]</span></p></li>
<li><p>Multivariate: <span class="math inline">\(f(X) = \sum_i{\beta_i X^{(i)}} + \sum_{i,j}{\beta_{i,j} X^{(i)} X^{(j)}}+\sum_{i,j,k}{\beta_{i,j,k} X^{(i)} X^{(j)}X^{(k)}}+\dots\)</span></p></li>
</ul>
<h2 id="bias---vairance-tradeoff"><span class="header-section-number">4.1</span> Bias - Vairance Tradeoff</h2>
<ul>
<li><p>Large bias, small variance: poor approximation but robust/stable</p></li>
<li><p>Small bias, large variance: good approximation but unstable</p></li>
</ul>
<p>Bias-Variance Decomposition: <span class="math display">\[E[(f(X)-f^*(X))^2] = Bias^2 + Variance\]</span></p>
<ul>
<li><p><span class="math inline">\(Bias = E[f(X)] - f^*(X)\)</span>: How far is the model from best model.</p></li>
<li><p><span class="math inline">\(Variance = E[(f(X)-E[f(X)])^2]\)</span>: How variable is the model.</p></li>
</ul>
<p><img src="./img/testerror.png" alt="image" width="302" /> <img src="./img/trainerror.png" alt="image" width="302" /></p>
<h1 id="regression-with-basis-functions-or-nonlinear-features"><span class="header-section-number">5</span> Regression with Basis Functions or Nonlinear Features</h1>
<p><span class="math display">\[f(X)=\sum_i \beta_i \phi_i(X)\]</span></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A function is called convex if the line joining any two points on the function does not go below the function on the interval formed by these two points.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>In contrast to the positive-definite case, these vectors need not be linearly independent.<a href="#fnref2">↩</a></p></li>
</ol>
</section>
</body>
</html>
